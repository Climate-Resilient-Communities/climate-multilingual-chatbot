# Cache Fix Deployment Checklist

## Status: ‚úÖ Code is READY but may not be DEPLOYED

### What Was Fixed:
Removed `model_type` from cache KEY (moved to VALUE) so cache works across model routing changes.

### Expected Cache Key for Test Question:
```
Question: "What are the local impacts of climate change in Toronto?"
Cache key: q:en:41d95a83486a6ab486548d9f3c506a59568a6ebd7cb7d7a617e1177fdad915d7
```

---

## ‚ö†Ô∏è DEPLOYMENT CHECKLIST

### 1. Verify Code is Deployed
**On your production server, run:**
```bash
cd /path/to/climate-multilingual-chatbot
git log --oneline -1
```

**Expected:** Should show commit `b398277` or later
**If not:** The fix is NOT deployed yet!

---

### 2. Verify Code Changes Are Active
**On production server, check the actual code:**
```bash
grep -A2 "def _make_cache_key" src/models/climate_pipeline.py
```

**Expected output:**
```python
def _make_cache_key(self, language_code: str, base_query: str) -> str:
```

**If you see `model_type` parameter:** OLD code is running!

---

### 3. Restart Application
**After deploying new code, you MUST restart:**
```bash
# For Docker
docker-compose restart

# For systemd service
sudo systemctl restart climate-chatbot

# For PM2
pm2 restart climate-chatbot

# For gunicorn/uvicorn
pkill -HUP gunicorn  # or kill and restart
```

‚ö†Ô∏è **Critical:** Python loads code at startup. If you deployed new code but didn't restart, the OLD code is still running in memory!

---

### 4. Test Cache Key Generation
**On production, run:**
```bash
python3 debug_specific_question.py
```

**Expected:** Should show cache key ending in `41d95a83...`

---

### 5. Check Redis Connection
**On production:**
```bash
redis-cli PING
# Should return: PONG

redis-cli KEYS "q:en:*" | wc -l
# Shows number of English query cache keys
```

---

### 6. Check If Question is Cached
**On production:**
```bash
redis-cli GET "q:en:41d95a83486a6ab486548d9f3c506a59568a6ebd7cb7d7a617e1177fdad915d7"
```

**If returns `(nil)`:**
- Question has NEVER been answered with new code
- Need to ask it once to populate cache

**If returns JSON data:**
- Question IS cached!
- If still getting cache miss, check logs for errors

---

### 7. Check Application Logs
**When asking the question, watch logs for:**
```bash
tail -f /var/log/climate-chatbot/app.log
# or
docker logs -f climate-chatbot
```

**Look for:**
- `"‚úì Cache hit for english"` ‚Üê Should see this!
- `"Cache miss for english query"` ‚Üê Problem if seeing this
- `"Cache initialization failed"` ‚Üê Redis connection issue
- `"Cache unavailable"` ‚Üê Redis error

---

## üö® Common Issues

### Issue 1: "Cache miss" even though Redis key exists
**Cause:** Old code still running (not restarted after deploy)
**Fix:**
```bash
# Restart the application
docker-compose restart
# or
sudo systemctl restart climate-chatbot
```

### Issue 2: Question never in Redis
**Cause:** Question never answered with new cache key format
**Fix:** Ask the question once - it will cache with new key

### Issue 3: Cache keys keep changing
**Cause:** `FORCE_COMMAND_A_RESPONSES` env var changing
**Fix:** This is now OK! Same key regardless of model routing

### Issue 4: Redis connection failing
**Cause:** Redis env vars not set or Redis down
**Check:**
```bash
echo $REDIS_HOST
echo $REDIS_PORT
redis-cli -h $REDIS_HOST -p $REDIS_PORT PING
```

---

## üìä Expected Behavior After Fix

### First Request (New User):
```
User: "What are the local impacts of climate change in Toronto?"
‚Üí Cache check: MISS (not in Redis yet)
‚Üí Generate response with model_type='nova'
‚Üí Store in Redis with key: q:en:41d95a83...
‚Üí Value contains: {response, model_type: 'nova', ...}
‚Üí Response time: 5-10 seconds
```

### Second Request (Different User, 5 minutes later):
```
User: "What are the local impacts of climate change in Toronto?"
‚Üí Cache check: HIT! (found in Redis)
‚Üí Return cached response (generated by nova)
‚Üí Log: "‚úì Cache hit for english"
‚Üí Response time: <100ms ‚ö°
```

### Third Request (model routing changed to cohere):
```
User: "What are the local impacts of climate change in Toronto?"
‚Üí Cache check: HIT! (SAME cache key despite different model!)
‚Üí Return cached response
‚Üí Response time: <100ms ‚ö°
‚Üí This is the FIX - it used to be a cache MISS!
```

---

## üîç Debugging Commands

### Check what code is actually running:
```bash
# Find the process
ps aux | grep python | grep climate

# Check what file it's using
lsof -p <PID> | grep climate_pipeline.py

# Verify Python is reading new code
python3 -c "import sys; sys.path.insert(0, 'src'); from models.climate_pipeline import ClimatePipeline; import inspect; print(inspect.signature(ClimatePipeline._make_cache_key))"
```

### Force flush Redis (nuclear option):
```bash
redis-cli FLUSHDB  # ‚ö†Ô∏è Deletes ALL cache!
```

### Watch cache in real-time:
```bash
redis-cli MONITOR | grep "q:en:"
```

---

## ‚úÖ Success Criteria

You'll know it's working when:
1. ‚úÖ Logs show: `"‚úì Cache hit for english"` on second request
2. ‚úÖ Same question takes <100ms after first request
3. ‚úÖ Redis has the key: `redis-cli EXISTS "q:en:41d95a83...` returns 1
4. ‚úÖ Works even if `FORCE_COMMAND_A_RESPONSES` changes

---

## üìû Still Not Working?

Run this on production and send output:
```bash
python3 diagnose_deployment.py > diagnosis.txt 2>&1
cat diagnosis.txt
```

This will show exactly what's wrong.
